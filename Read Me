Model Coefficients are saved in my_model2.h5


The Process to work with this code.
1. First download the dataset. (If ylimit error is not shown, you can use yfinance to have the data set)
The follwing codes can be used to download date in CSV formate from yfinance
import yfinance as yf

# Download Microsoft (MSFT) historical stock data from Yahoo Finance
msft_data = yf.download("MSFT", start="2010-01-01", end="2023-12-31")

# Display the downloaded data
print(msft_data)



2. Load training/validation/test data from CSV.
Convert DataFrames to NumPy arrays.
Apply standard scaling to both features and labels.
Model Construction:

3. A 5-layer (each 18 neurons) feedforward network with ReLU activations, BatchNorm, and Dropout (0.3) after each dense layer.
A final Dense(1) for regression output.
Adam optimizer with an initial LR of 1e-5 and MSE loss.
Cyclical LR Scheduler:

4. The learning rate is adjusted each epoch using the triangular2 method. It oscillates between 1e-5 and 1e-2 over step_size=10 epochs, then reduces the amplitude each cycle.
Training:

5. Train for 250 epochs (validation data used for real-time feedback).
The LR scheduler is applied each epoch.
Evaluation:

6. Evaluate MSE on the test set in scaled space and convert it back to the original space.
Display a few predictions vs. actual values.
Plotting & Saving:

7. Plot training and validation loss over epochs to visualize convergence.
Save the trained model to an .h5 file for future use.
This is essentially a deep regression pipeline that uses:

8. Data splitting (train, val, test),
Scaling (important for neural networks),
A custom cyclical learning rate (for potentially better convergence),
A moderately deep architecture (5 hidden layers of 18 neurons),
Regularization (Dropout + BatchNorm),
Adam optimizer,
MSE loss.
All of these steps together constitute the full training and evaluation procedure seen in the code.
